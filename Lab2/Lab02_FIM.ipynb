{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "DSYvP0n8RjYH"
   },
   "source": [
    "# Lab02: Frequent itemset mining\n",
    "\n",
    "- Student ID: 20127586\n",
    "- Student name: Nguyễn Đình Pháp\n",
    "\n",
    "**How to do your homework**\n",
    "\n",
    "\n",
    "You will work directly on this notebook; the word `TODO` indicate the parts you need to do.\n",
    "\n",
    "You can discuss ideas with classmates as well as finding information from the internet, book, etc...; but *this homework must be your*.\n",
    "\n",
    "**How to submit your homework**\n",
    "\n",
    "Before submitting, rerun the notebook (`Kernel` ->` Restart & Run All`).\n",
    "\n",
    "Then create a folder named `ID` (for example, if your ID is 1234567, then name the folder `1234567`) Copy file notebook to this folder, compress and submit it on moodle.\n",
    "\n",
    "**Contents:**\n",
    "\n",
    "- Frequent itemset mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXZ5gCVaRjYa"
   },
   "source": [
    "# 1. Preliminaries\n",
    "## This is how it all started ...\n",
    "- Rakesh Agrawal, Tomasz Imielinski, Arun N. Swami: Mining Association Rules between Sets of Items in Large Databases. SIGMOD Conference 1993: 207-216\n",
    "- Rakesh Agrawal, Ramakrishnan Srikant: Fast Algorithms for Mining Association Rules in Large Databases. VLDB 1994: 487-499\n",
    "\n",
    "**These two papers are credited with the birth of Data Mining**\n",
    "## Frequent itemset mining (FIM)\n",
    "\n",
    "Find combinations of items (itemsets) that occur frequently.\n",
    "## Applications\n",
    "- Items = products, transactions = sets of products someone bought in one trip to the store.\n",
    "$\\Rightarrow$ items people frequently buy together.\n",
    "    + Example: if people usually buy bread and coffee together, we run a sale of bread to attract people attention and raise price of coffee.\n",
    "- Items = webpages, transactions = words. Unusual words appearing together in a large number of documents, e.g., “Brad” and “Angelina,” may indicate an interesting relationship.\n",
    "- Transactions = Sentences, Items = Documents containing those sentences. Items that appear together too often could represent plagiarism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8vAJ8A2RjYi"
   },
   "source": [
    "## Transactional Database\n",
    "A transactional database $D$ consists of $N$ transactions: $D=\\left\\{T_1,T_2,...,T_N\\right\\}$. A transaction $T_n \\in D (1 \\le n \\le N)$ contains one or more items and that $I= \\left\\{ i_1,i_2,…,i_M \\right\\}$ is the set of distinct items in $D$, $T_n \\subset I$. Commonly, a transactional database is represented by a flat file instead of a database system: items are non-negative integers, each row represents a transaction, items in a transaction separated by space.\n",
    "\n",
    "Example: \n",
    "\n",
    "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 \n",
    "\n",
    "30 31 32 \n",
    "\n",
    "33 34 35 \n",
    "\n",
    "36 37 38 39 40 41 42 43 44 45 46 \n",
    "\n",
    "38 39 47 48 \n",
    "\n",
    "38 39 48 49 50 51 52 53 54 55 56 57 58 \n",
    "\n",
    "32 41 59 60 61 62 \n",
    "\n",
    "3 39 48 \n",
    "\n",
    "63 64 65 66 67 68 \n",
    "\n",
    "\n",
    "\n",
    "# Definition\n",
    "\n",
    "- Itemset: A collection of one or more items.\n",
    "    + Example: {1 4 5}\n",
    "- **k-itemset**: An itemset that contains k items.\n",
    "- Support: Frequency of occurrence of an itemset.\n",
    "    + Example: From the example above, item 3 appear in 2 transactions so its support is 2.\n",
    "- Frequent itemset: An itemset whose support is greater than or equal to a `minsup` threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdykKxr6RjY-"
   },
   "source": [
    "# The Apriori Principle\n",
    "- If an itemset is frequent, then all of its subsets must also be frequent.\n",
    "- If an itemset is not frequent, then all of its supersets cannot be frequent.\n",
    "- The support of an itemset never exceeds the support of its subsets.\n",
    "$$ \\forall{X,Y}: (X \\subseteq Y) \\Rightarrow s(X)\\ge s(Y)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvfMR7-CRjZB"
   },
   "source": [
    "# 2. Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9gZh4DORjZD"
   },
   "source": [
    "## The Apriori algorithm\n",
    "Suppose:\n",
    "\n",
    "$C_k$ candidate itemsets of size k.\n",
    "\n",
    "$L_k$ frequent itemsets of size k.\n",
    "\n",
    "The level-wise approach of Apriori algorithm can be descibed as follow:\n",
    "1. k=1, $C_k$ = all items.\n",
    "2. While $C_k$ not empty:\n",
    "    3. Scan the database to find which itemsets in $C_k$ are frequent and put them into $L_k$.\n",
    "    4. Use $L_k$ to generate a collection of candidate itemsets $C_{k+1}$ of size k+1.\n",
    "    5. k=k+1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qF9xHOBLRjZJ"
   },
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7F0lUOSuRjZN"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OogwdcLRjZf"
   },
   "source": [
    "### Read data\n",
    "First we have to read data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2bsGrTERjZg"
   },
   "outputs": [],
   "source": [
    "\n",
    "def readData(path):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------\n",
    "        path: path of database D.\n",
    "         \n",
    "    --------------------------\n",
    "    Returns\n",
    "        data: a dictionary for representing database D\n",
    "                 - keys: transaction tids\n",
    "                 - values: itemsets.\n",
    "        s: support of distict items in D.\n",
    "    \"\"\"\n",
    "    data={}\n",
    "    s=defaultdict(lambda: 0) # Initialize a dictionary for storing support of items in I.  \n",
    "    with open(path,'rt') as f:\n",
    "        tid=1;\n",
    "        for line in f:\n",
    "            itemset=set(map(int,line.split())) # a python set is a native way for storing an itemset.\n",
    "            for item in itemset:  \n",
    "                s[item]+=1     #Why don't we compute support of items while reading data?\n",
    "            data[tid]= itemset\n",
    "            tid+=1\n",
    "    \n",
    "    return data, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSTC78WURjZu"
   },
   "source": [
    "### Tree Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGAkmuXtRjZw"
   },
   "source": [
    "**I gave you pseudo code of Apriori algorithm above but we implement Tree Projection. Tell me the differences of two algorithms.**\n",
    "\n",
    "\n",
    "**TODO:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BVRT5BnWRjZz"
   },
   "outputs": [],
   "source": [
    "def joinset(a, b):\n",
    "    '''\n",
    "    Parameters\n",
    "    -------------------\n",
    "        2 itemsets a and b (of course they are at same branch in search space)\n",
    "\n",
    "    -------------------\n",
    "    return\n",
    "        ret: itemset generated by joining a and b\n",
    "    '''\n",
    "    # TODO (hint: this function will be called in generateSearchSpace method.):\n",
    "    #ret = set(a).union(set(b))\n",
    "    ret = list(set(a) | set(b))\n",
    "    return ret\n",
    "\n",
    "class TP:\n",
    "    def __init__(self, data=None, s=None, minSup=None):\n",
    "        self.data = data\n",
    "        self.s = {}\n",
    "\n",
    "        for key, support in sorted(s.items(), key=lambda item: item[1]):\n",
    "            self.s[key] = support\n",
    "        # TODO: why should we do this, answer it at the markdown below?\n",
    "\n",
    "        self.minSup = minSup\n",
    "        self.L = {}  # Store frequent itemsets mined from database\n",
    "        self.runAlgorithm()\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"\n",
    "        Initialize search space at first step\n",
    "        --------------------------------------\n",
    "        We represent our search space in a tree structure\n",
    "        \"\"\"\n",
    "        tree = {}\n",
    "\n",
    "        search_space = {}\n",
    "        for item, support in self.s.items():\n",
    "            search_space[item] = {}\n",
    "\n",
    "            search_space[item]['itemset'] = [item]\n",
    "            ''' \n",
    "            python set does not remain elements order\n",
    "            so we use a list to extend it easily when create new itemset \n",
    "            but why we store itemset in data by a python set???? '''\n",
    "            # TODO: study about python set and its advantages,\n",
    "            # answer at the markdown below.\n",
    "\n",
    "            search_space[item]['pruned'] = False\n",
    "            # TODO:\n",
    "            # After finish implementing the algorithm tell me why should you use this\n",
    "            # instead of delete item directly from search_space and tree.\n",
    "\n",
    "            search_space[item]['support'] = support\n",
    "\n",
    "            tree[item] = {}\n",
    "            '''\n",
    "            Why should i store an additional tree (here it called tree)? \n",
    "            Answer: This really help in next steps.\n",
    "\n",
    "            Remember that there is always a big gap from theory to practicality\n",
    "            and implementing this algorithm in python is not as simple as you think.\n",
    "            '''\n",
    "\n",
    "        return tree, search_space\n",
    "\n",
    "    def computeItemsetSupport(self, itemset):\n",
    "\n",
    "        '''Return support of itemset'''\n",
    "        # TODO (hint: this is why i use python set in data)\n",
    "        #support = float('inf')\n",
    "        # for item in itemset:\n",
    "        #     if self.s[item] < support:\n",
    "        #         support = self.s[item]\n",
    "        support = self.s[itemset] / len(self.data)\n",
    "        return support\n",
    "\n",
    "    def get_sub_tree(self, k, tree, search_space, itter_node):\n",
    "        if k == 0:\n",
    "            return search_space[itter_node]['support']\n",
    "        subtree = search_space[itter_node]\n",
    "        for node in subtree.keys():\n",
    "            k-=1\n",
    "            self.get_sub_tree(k,tree,search_space,node)\n",
    "\n",
    "\n",
    "    def prune(self, k, tree, search_space):\n",
    "\n",
    "        '''\n",
    "        In this method we will find out which itemset in current search space is frequent\n",
    "        itemset then add it to L[k]. In addition, we prune those are not frequent itemsets.\n",
    "        '''\n",
    "        if self.L.get(k) is None: self.L[k] = []\n",
    "        # TODO\n",
    "        items = list(tree.keys())\n",
    "        for item in items:\n",
    "            support = self.computeItemsetSupport(item)\n",
    "            compute_sup = self.minSup / len(self.data)\n",
    "            if support >= compute_sup:\n",
    "                self.L[k].append(item)\n",
    "                search_space[item]['pruned'] = False\n",
    "            else:\n",
    "                self.L[k] = []\n",
    "                search_space[item]['pruned'] = True\n",
    "\n",
    "\n",
    "    def generateSearchSpace(self, k, tree, search_space):\n",
    "        '''\n",
    "        Generate search space for exploring k+1 itemset. (Recursive function)\n",
    "        '''\n",
    "        items = list(tree.keys())\n",
    "        ''' print search_space.keys() you will understand  \n",
    "         why we need an additional tree, '''\n",
    "        l = len(items)\n",
    "        self.prune(k, tree, search_space)\n",
    "        if l == 0: return  # Stop condition\n",
    "        for i in range(l - 1):\n",
    "            sub_search_space = {}\n",
    "            sub_tree = {}\n",
    "            a = items[i]\n",
    "            if search_space[a]['pruned']: continue\n",
    "\n",
    "            for j in range(i + 1, l):\n",
    "                b = items[j]\n",
    "                search_space[a][b] = {}\n",
    "                tree[a][b] = {}\n",
    "                # You really need to understand what am i doing here before doing work below.\n",
    "                # (Hint: draw tree and search space to draft).\n",
    "\n",
    "                # TODO:\n",
    "                # First create newset using join set\n",
    "                new_l1, new_l2 = [], []\n",
    "                new_l1.append(a)\n",
    "                new_l2.append(b)\n",
    "                new_set = joinset(new_l1, new_l2)\n",
    "                new_set = set(new_set)\n",
    "                \n",
    "                # Second add newset to search_space\n",
    "                search_space[a][b] = new_set\n",
    "                \n",
    "            #  Generate search_space for k+1-itemset\n",
    "            self.generateSearchSpace(k + 1, sub_tree, sub_search_space)\n",
    "\n",
    "    def runAlgorithm(self):\n",
    "        tree, search_space = self.initialize()  # generate search space for 1-itemset\n",
    "        self.generateSearchSpace(1, tree, search_space)\n",
    "\n",
    "    def miningResults(self):\n",
    "        return self.L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tMTpwxLRjZ-"
   },
   "source": [
    "Ok, let's test on a typical dataset `chess`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gLygYqiYRjZ-"
   },
   "outputs": [],
   "source": [
    "data, s= readData('chess.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PnxbU77YRjaF",
    "outputId": "c3b158be-6b46-4a3c-9b71-6a92d3d31ded"
   },
   "outputs": [],
   "source": [
    "#\n",
    "a=TP(data=data,s=s, minSup=3000)\n",
    "print(a.miningResults())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Mp0RFbw-RjaU"
   },
   "source": [
    "### Answer questions here:\n",
    "**Why don't we compute support of items while reading data?**\n",
    "\n",
    "Calculating the support of items during reading is inefficient because during reading, we don't know what the specific frequent itemset is. Therefore, we must read through the entire data set at least twice, once to find frequent itemsets and once to compute their support.\n",
    "\n",
    "**why should we do sort**\n",
    "\n",
    "Sorting the data set makes it easier to find frequent itemsets. When the data set is sorted, items of the same type will be located close to each other, making the support calculation of itemsets faster. In addition, sorting also makes it easier to build an FP-growth tree.\n",
    "\n",
    "**study about python set and its advantages ?**\n",
    "\n",
    "In Python, a set is a built-in data type that represents a collection of unique elements. Sets offer several advantages, such as constant-time membership testing, fast union and intersection operations, and the ability to remove duplicates from a list. These properties make sets useful for tasks such as finding unique elements in a list, testing set membership, and implementing mathematical operations on sets.\n",
    "\n",
    "**After finish implementing the algorithm tell me why should you use this? Instead of delete item directly from search_space and tree.**\n",
    "\n",
    "Deleting an item directly from the search space and tree can be computationally expensive, especially for large datasets. Instead, the Apriori algorithm prunes the search space by using the property that all non-frequent itemsets must contain at least one non-frequent subset. By avoiding unnecessary computations and reducing the size of the search space, the Apriori algorithm can be much faster and more efficient than deleting items directly.\n",
    "\n",
    "**Apriori algorithm and Tree Projection, tell me the differences of two algorithms.**\n",
    "\n",
    "The Apriori algorithm and Tree Projection are two algorithms used in frequent itemset mining. The Apriori algorithm works by iteratively generating candidate itemsets and counting their support, while Tree Projection works by projecting a transaction database onto a conditional pattern base and recursively constructing a conditional FP-tree.\n",
    "The main difference between the two algorithms is that the Apriori algorithm generates candidate itemsets by joining two frequent itemsets, while Tree Projection recursively constructs a conditional FP-tree by projecting the transaction database onto a frequent itemset. The Apriori algorithm requires multiple passes over the transaction database to generate frequent itemsets, while Tree Projection requires a single pass to construct the FP-tree. \n",
    "--> Overall, the Apriori algorithm is simpler and easier to implement, while Tree Projection can be more efficient for large datasets and can handle more complex patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnVm8wYIRjaV"
   },
   "source": [
    "# 3. Churn analysis\n",
    "\n",
    "In this section, you will use frequent itemset mining technique to analyze `churn` dataset (for any purposes). \n",
    "\n",
    "*Remember this dataset is not represented as a transactional database, first thing that you have to do is transforming it into a flat file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a csv file from the html table\n",
    "with open('churn.txt', 'r') as input_file:\n",
    "    soup = BeautifulSoup(input_file.read(), 'html.parser')\n",
    "    table = soup.find('table')\n",
    "    with open('churn_file.csv', 'w', newline='') as output_file:\n",
    "        writer = csv.writer(output_file, delimiter=';')\n",
    "        for row in table.find_all('tr'):\n",
    "            cells = row.find_all('td')\n",
    "            writer.writerow([cell.get_text(strip=True) for cell in cells])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('churn_file.csv', 'r') as input_file:\n",
    "    # Đọc nội dung của file\n",
    "    content = input_file.read()\n",
    "    # Loại bỏ ký tự \";\" ở đầu và \".\" ở cuối mỗi dòng\n",
    "    content = content.replace(';', '')\n",
    "    \n",
    "with open('churn_file.csv', 'w') as output_file:\n",
    "    # Ghi nội dung đã loại bỏ ký tự vào file mới\n",
    "    output_file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile():\n",
    "    f = open(\"churn_file.csv\", 'r')\n",
    "    for line in f:\n",
    "        data = line.split(',')\n",
    "        data[0] = \"State: \" + data[0]\n",
    "        data[1] = \"Account Length: \" + data[1]\n",
    "        data[2] = \"Area Code: \" + data[2]\n",
    "        data[3] = \"Phone: \" + data[3]\n",
    "        data[4] = \"Int'l Plan: \" + data[4]\n",
    "        data[5] = \"VMail Plan: \" + data[5]\n",
    "        data[6] = \"VMail Message: \" + data[6]\n",
    "        data[7] = \"Day Mins: \" + data[7]\n",
    "        data[8] = \"Day Calls: \" + data[8]\n",
    "        data[9] = \"Day Charge: \" + data[9]\n",
    "        data[10] = \"Eve Mins: \" + data[10]\n",
    "        data[11] = \"Eve Calls: \" + data[11]\n",
    "        data[12] = \"Eve Charge: \" + data[12]\n",
    "        data[13] = \"Night Mins: \" + data[13]\n",
    "        data[14] = \"Night Calls: \" + data[14]\n",
    "        data[15] = \"Night Charge: \" + data[15]\n",
    "        data[16] = \"Intl Mins: \" + data[16]\n",
    "        data[17] = \"Intl Calls: \" + data[17]\n",
    "        data[18] = \"Intl Charge: \" + data[18]\n",
    "        data[19] = \"CustServ Calls: \" + data[19]\n",
    "        data[20] = \"Churn?: \" + data[20]\n",
    "        if not data: break\n",
    "        yield data\n",
    "\n",
    "data = readFile()\n",
    "\n",
    "class churnAnalysis:\n",
    "    def __init__(self):\n",
    "        self.minSup = 2000 #0.6\n",
    "        self.rowTransaction = set() #all data of each row in dataset\n",
    "        self.L = defaultdict(int) #store number of a itemset for computing its support\n",
    "        \n",
    "    def candidateC1(self):\n",
    "        itemSet = set()\n",
    "        for i in data:\n",
    "            tid = frozenset(i)\n",
    "            self.rowTransaction.add(tid)\n",
    "            for item in tid: itemSet.add(frozenset([item])) # use frozenset as a key in a dict\n",
    "        sorted(itemSet, reverse=False)\n",
    "        return itemSet\n",
    "    \n",
    "    def itemWithMinsup(self, Ck):\n",
    "        prune = set()\n",
    "        cnt = defaultdict(int)\n",
    "        for tid in self.rowTransaction:\n",
    "            for item in Ck:\n",
    "                if item.issubset(tid):\n",
    "                    self.L[item] += 1\n",
    "                    cnt[item] += 1\n",
    "        for key, support in cnt.items():\n",
    "            sup = support / len(self.rowTransaction)\n",
    "            convertMinsup = round(self.minSup / len(self.rowTransaction), 2)\n",
    "            if sup >= convertMinsup: prune.add(key)\n",
    "            else: continue \n",
    "        return prune\n",
    "    \n",
    "    def computeSupport(self, item):\n",
    "        return self.L.get(item) / len(self.rowTransaction)\n",
    "    \n",
    "    def selfjoin(self, itemset):\n",
    "        Ck = set()\n",
    "        for a in itemset:\n",
    "            for b in itemset:\n",
    "                sorted(a, reverse=False)\n",
    "                sorted(b, reverse=False)\n",
    "                c = a & b \n",
    "                if(len(c) == len(a) - 1): Ck.add(a | b)\n",
    "        return Ck\n",
    "    \n",
    "    def Apriori(self):\n",
    "        aprioriGen = dict()\n",
    "        C1 = self.candidateC1()\n",
    "        Lk = self.itemWithMinsup(C1)\n",
    "        k = 2\n",
    "        if(len(Lk) == 0): return\n",
    "        while(len(Lk) != 0):\n",
    "            aprioriGen[k - 1] = Lk\n",
    "            new_join = self.selfjoin(Lk)\n",
    "            scanDB = self.itemWithMinsup(new_join)\n",
    "            Lk = scanDB\n",
    "            k += 1\n",
    "        result = []\n",
    "        for key, value in aprioriGen.items():\n",
    "            result.extend([(set(item), self.computeSupport(item)) for item in value])\n",
    "        for key, support in sorted(result, key=lambda item: item[1]):\n",
    "            print(str(key) + ' with support: ' + str(round(support, 2)))\n",
    "\n",
    "a = churnAnalysis()\n",
    "a.Apriori()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### from result above, we have total 12 frequent itemsets with minSup >= 2000\n",
    "\n",
    "Describe:\n",
    "\n",
    "- I read the data and transform it into a flat file. \n",
    "\n",
    "    Then i use apriori algorithm for analysing churn purpose\n",
    "\n",
    "- candidateC1(self) : In the function, we create all possible C1 candidates from our dataset using frozenset instead of set. This is because we will use these sets as dictionary keys, and thus they need to be immutable. We then count the support of each candidate, and add it to the C1 dictionary if it is greater than the minimum support. We also add the candidate to the L1 list if it is frequent. We then return the C1 dictionary and the L1 list.\n",
    "\n",
    "- itemWithMinsup(self, Ck) : In this function, we will have list frequent L1 from list of candidate C1 we created from function candidateC1(self), then we take itemsets satisfy our minSup. \n",
    "\n",
    "- computeSupport(self, item): find support of a item in dataset\n",
    "\n",
    "- selfjoin(self, itemset): this function will take list of frequent itemset Lk to produce Ck. Example our L1 is {a}, {b}, {c} -> {a, b}, {a, c}, {b, c} and two set are combined using union which is the \"|\" symbol.\n",
    "\n",
    "- apriori(self): this is the main function of the Apriori algorithm. It will call the candidateC1(self) function to create the C1 dictionary and L1 list, and then it will call the itemWithMinsup(self, Ck) function to create the L1 list from the C1 dictionary. It will then continue to call the selfjoin(self, itemset) function to create the Ck and Lk lists until Lk is empty. It will then return the L1 list and the Lk list.\n",
    "\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this lab02, I have learned how to implement the Apriori algorithm and use it to find frequent itemsets in a dataset. I have also learned how to use the Apriori algorithm to generate association rules from a transactional dataset. I have learned that the Apriori algorithm is a simple and effective algorithm for frequent itemset mining, and it can be used to find interesting patterns in transactional datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FzxGs7RRjaX"
   },
   "source": [
    "# 4 References"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "doYH4biqR_N7"
   },
   "source": [
    "Feel free to send questions to my email address: ntthuhang0131@gmail.com\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Lab01 - Frequent itemset mining.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
